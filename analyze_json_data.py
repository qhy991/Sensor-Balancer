#!/usr/bin/env python3
"""
åˆ†æJSONæ ¼å¼çš„ä½ç½®ä¸€è‡´æ€§æ•°æ®
"""

import json
import os
import sys
import numpy as np
from datetime import datetime

def load_data_from_json(filepath):
    """ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {filepath}")
        return data
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {filepath}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ JSONæ ¼å¼é”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def analyze_consistency_data(data):
    """åˆ†æä¸€è‡´æ€§æ•°æ®"""
    print("ğŸ”§ å¼€å§‹åˆ†æä½ç½®ä¸€è‡´æ€§æ•°æ®")
    print("=" * 60)
    
    # åŸºæœ¬ä¿¡æ¯
    timestamp = data.get('timestamp', 'æœªçŸ¥')
    guide_positions = data.get('guide_positions', {})
    consistency_results = data.get('consistency_results', {})
    
    print(f"ğŸ“… æ•°æ®æ—¶é—´: {timestamp}")
    print(f"ğŸ“ æ€»ä½ç½®æ•°: {len(guide_positions)}")
    print(f"ğŸ“Š æœ‰æ•°æ®çš„ä½ç½®æ•°: {len(consistency_results)}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    all_sensitivities = []
    all_cvs = []
    position_stats = {}
    
    print("\nğŸ¯ ä½ç½®è¯¦ç»†åˆ†æ:")
    print("-" * 40)
    
    for pos_id, pos_data in consistency_results.items():
        pos_name = guide_positions.get(pos_id, {}).get('name', pos_id)
        sensitivities = []
        cvs = []
        
        print(f"\nğŸ“ {pos_name} ({pos_id}):")
        
        for weight_id, result in pos_data.items():
            sensitivity = result.get('sensitivity_total', 0)
            cv = result.get('cv', 0)
            mass = result.get('weight_info', {}).get('mass', 0)
            
            sensitivities.append(sensitivity)
            cvs.append(cv)
            all_sensitivities.append(sensitivity)
            all_cvs.append(cv)
            
            print(f"   ç ç {weight_id} ({mass}g): æ•æ„Ÿæ€§={sensitivity:.6f}, CV={cv:.6f}")
        
        # ä½ç½®ç»Ÿè®¡
        if sensitivities:
            avg_sensitivity = np.mean(sensitivities)
            avg_cv = np.mean(cvs)
            position_stats[pos_id] = {
                'name': pos_name,
                'avg_sensitivity': avg_sensitivity,
                'avg_cv': avg_cv,
                'sensitivities': sensitivities,
                'cvs': cvs
            }
            print(f"   å¹³å‡: æ•æ„Ÿæ€§={avg_sensitivity:.6f}, CV={avg_cv:.6f}")
    
    # æ€»ä½“ç»Ÿè®¡
    print("\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print("-" * 40)
    
    if all_sensitivities:
        avg_sensitivity = np.mean(all_sensitivities)
        std_sensitivity = np.std(all_sensitivities)
        min_sensitivity = np.min(all_sensitivities)
        max_sensitivity = np.max(all_sensitivities)
        
        avg_cv = np.mean(all_cvs)
        std_cv = np.std(all_cvs)
        
        print(f"å¹³å‡æ•æ„Ÿæ€§: {avg_sensitivity:.6f} Â± {std_sensitivity:.6f}")
        print(f"æ•æ„Ÿæ€§èŒƒå›´: [{min_sensitivity:.6f}, {max_sensitivity:.6f}]")
        print(f"å¹³å‡å˜å¼‚ç³»æ•°: {avg_cv:.6f} Â± {std_cv:.6f}")
        
        # ä½ç½®é—´ä¸€è‡´æ€§
        if len(position_stats) > 1:
            position_sensitivities = [stats['avg_sensitivity'] for stats in position_stats.values()]
            position_consistency_cv = np.std(position_sensitivities) / np.mean(position_sensitivities)
            print(f"ä½ç½®é—´ä¸€è‡´æ€§CV: {position_consistency_cv:.3f}")
            
            # ä¸€è‡´æ€§è¯„ä¼°
            if position_consistency_cv < 0.05:
                consistency_level = "ä¼˜ç§€ (<5%)"
            elif position_consistency_cv < 0.10:
                consistency_level = "è‰¯å¥½ (5-10%)"
            elif position_consistency_cv < 0.20:
                consistency_level = "ä¸€èˆ¬ (10-20%)"
            else:
                consistency_level = "éœ€è¦æ”¹è¿› (>20%)"
            
            print(f"ä½ç½®ä¸€è‡´æ€§: {consistency_level}")
    
    # é—®é¢˜åˆ†æ
    print("\nğŸ” é—®é¢˜åˆ†æ:")
    print("-" * 40)
    
    # æ£€æŸ¥è´Ÿæ•æ„Ÿæ€§
    negative_sensitivities = [s for s in all_sensitivities if s < -1e-6]
    if negative_sensitivities:
        print(f"âš ï¸ å‘ç° {len(negative_sensitivities)} ä¸ªè´Ÿæ•æ„Ÿæ€§å€¼")
        for pos_id, stats in position_stats.items():
            for i, sensitivity in enumerate(stats['sensitivities']):
                if sensitivity < -1e-6:
                    print(f"   {stats['name']} ç ç {i+1}: {sensitivity:.6f}")
    else:
        print("âœ… æœªå‘ç°è´Ÿæ•æ„Ÿæ€§å€¼")
    
    # æ£€æŸ¥é«˜CVå€¼
    high_cv_threshold = 0.1
    high_cvs = [cv for cv in all_cvs if cv > high_cv_threshold]
    if high_cvs:
        print(f"âš ï¸ å‘ç° {len(high_cvs)} ä¸ªé«˜CVå€¼ (>10%)")
        for pos_id, stats in position_stats.items():
            for i, cv in enumerate(stats['cvs']):
                if cv > high_cv_threshold:
                    print(f"   {stats['name']} ç ç {i+1}: CV={cv:.6f}")
    else:
        print("âœ… æ‰€æœ‰CVå€¼éƒ½åœ¨æ­£å¸¸èŒƒå›´å†…")
    
    # æ£€æŸ¥ç¼ºå¤±ä½ç½®
    missing_positions = set(guide_positions.keys()) - set(consistency_results.keys())
    if missing_positions:
        print(f"âš ï¸ å‘ç° {len(missing_positions)} ä¸ªç¼ºå¤±ä½ç½®:")
        for pos_id in missing_positions:
            pos_name = guide_positions.get(pos_id, {}).get('name', pos_id)
            print(f"   {pos_name} ({pos_id})")
    else:
        print("âœ… æ‰€æœ‰ä½ç½®éƒ½æœ‰æ•°æ®")
    
    # å»ºè®®
    print("\nğŸ’¡ å»ºè®®:")
    print("-" * 40)
    
    if negative_sensitivities:
        print("â€¢ è´Ÿæ•æ„Ÿæ€§é€šå¸¸è¡¨ç¤ºåŸºçº¿æ ¡æ­£è¿‡åº¦ï¼Œå»ºè®®æ£€æŸ¥åŸºçº¿æ•°æ®è®°å½•è¿‡ç¨‹")
        print("â€¢ å¯èƒ½éœ€è¦è°ƒæ•´åŸºçº¿æ ¡æ­£ç®—æ³•")
        print("â€¢ è€ƒè™‘é‡æ–°è¿›è¡ŒåŸºçº¿æµ‹é‡")
    
    if high_cvs:
        print("â€¢ é«˜CVå€¼è¡¨ç¤ºæµ‹é‡é‡å¤æ€§å·®ï¼Œå»ºè®®æ£€æŸ¥æµ‹é‡ç¯å¢ƒç¨³å®šæ€§")
        print("â€¢ å¯èƒ½éœ€è¦å¢åŠ æµ‹é‡æ¬¡æ•°æˆ–æ”¹å–„æµ‹é‡æ¡ä»¶")
    
    if missing_positions:
        print("â€¢ å»ºè®®è¡¥å……ç¼ºå¤±ä½ç½®çš„æµ‹é‡æ•°æ®ä»¥è·å¾—å®Œæ•´çš„ä¼ æ„Ÿå™¨ç‰¹æ€§")
    
    if len(position_stats) > 1:
        if position_consistency_cv > 0.20:
            print("â€¢ ä½ç½®é—´ä¸€è‡´æ€§è¾ƒå·®ï¼Œå»ºè®®æ£€æŸ¥ä¼ æ„Ÿå™¨å®‰è£…å’Œæ ¡å‡†")
        elif position_consistency_cv > 0.10:
            print("â€¢ ä½ç½®é—´ä¸€è‡´æ€§ä¸€èˆ¬ï¼Œå¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–")
        else:
            print("â€¢ ä½ç½®é—´ä¸€è‡´æ€§è‰¯å¥½ï¼Œä¼ æ„Ÿå™¨æ€§èƒ½ç¨³å®š")
    
    return {
        'timestamp': timestamp,
        'total_positions': len(guide_positions),
        'measured_positions': len(consistency_results),
        'total_measurements': len(all_sensitivities),
        'avg_sensitivity': avg_sensitivity if all_sensitivities else 0,
        'std_sensitivity': std_sensitivity if all_sensitivities else 0,
        'avg_cv': avg_cv if all_cvs else 0,
        'std_cv': std_cv if all_cvs else 0,
        'position_consistency_cv': position_consistency_cv if len(position_stats) > 1 else 0,
        'position_stats': position_stats,
        'negative_sensitivities': len(negative_sensitivities),
        'high_cvs': len(high_cvs),
        'missing_positions': len(missing_positions)
    }

def save_analysis_report(results, output_dir="analysis_results"):
    """ä¿å­˜åˆ†ææŠ¥å‘Š"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = os.path.join(output_dir, f"detailed_analysis_{timestamp}.txt")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("è¯¦ç»†ä½ç½®ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š\n")
        f.write("=" * 60 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ•°æ®æ—¶é—´: {results['timestamp']}\n\n")
        
        f.write("ğŸ“Š æ•°æ®æ¦‚è§ˆ\n")
        f.write("-" * 30 + "\n")
        f.write(f"æ€»ä½ç½®æ•°: {results['total_positions']}\n")
        f.write(f"æµ‹é‡ä½ç½®æ•°: {results['measured_positions']}\n")
        f.write(f"æ€»æµ‹é‡ç‚¹: {results['total_measurements']}\n\n")
        
        f.write("ğŸ“ˆ ç»Ÿè®¡ç»“æœ\n")
        f.write("-" * 30 + "\n")
        f.write(f"å¹³å‡æ•æ„Ÿæ€§: {results['avg_sensitivity']:.6f} Â± {results['std_sensitivity']:.6f}\n")
        f.write(f"å¹³å‡å˜å¼‚ç³»æ•°: {results['avg_cv']:.6f} Â± {results['std_cv']:.6f}\n")
        f.write(f"ä½ç½®é—´ä¸€è‡´æ€§CV: {results['position_consistency_cv']:.3f}\n\n")
        
        f.write("ğŸ¯ ä½ç½®è¯¦ç»†åˆ†æ\n")
        f.write("-" * 30 + "\n")
        for pos_id, stats in results['position_stats'].items():
            f.write(f"{stats['name']}: æ•æ„Ÿæ€§={stats['avg_sensitivity']:.6f}, CV={stats['avg_cv']:.6f}\n")
        
        f.write(f"\nğŸ” é—®é¢˜ç»Ÿè®¡\n")
        f.write("-" * 30 + "\n")
        f.write(f"è´Ÿæ•æ„Ÿæ€§æ•°é‡: {results['negative_sensitivities']}\n")
        f.write(f"é«˜CVå€¼æ•°é‡: {results['high_cvs']}\n")
        f.write(f"ç¼ºå¤±ä½ç½®æ•°é‡: {results['missing_positions']}\n")
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    return report_file

def main():
    """ä¸»å‡½æ•°"""
    # ä»å‘½ä»¤è¡Œå‚æ•°æˆ–é»˜è®¤è·¯å¾„è·å–æ–‡ä»¶è·¯å¾„
    if len(sys.argv) > 1:
        json_filepath = sys.argv[1]
    else:
        # é»˜è®¤æ–‡ä»¶è·¯å¾„
        json_filepath = r"C:\Users\84672\Documents\Research\balance-sensor\consistency-test\0801-2.json"
    
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶è·¯å¾„: {json_filepath}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(json_filepath):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_filepath}")
        print("è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæˆ–é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ–‡ä»¶è·¯å¾„")
        return
    
    # åŠ è½½æ•°æ®
    data = load_data_from_json(json_filepath)
    if data is None:
        return
    
    # åˆ†ææ•°æ®
    results = analyze_consistency_data(data)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = save_analysis_report(results)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼è¯¦ç»†æŠ¥å‘Š: {report_file}")

if __name__ == "__main__":
    main() 