#!/usr/bin/env python3
"""
ä½ç½®ä¸€è‡´æ€§æ•°æ®åˆ†æè„šæœ¬
ç”¨äºå¤„ç†å’Œåˆ†æä¼ æ„Ÿå™¨ä½ç½®ä¸€è‡´æ€§æµ‹è¯•æ•°æ®
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ"""
    try:
        # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
        print("âœ… ä¸­æ–‡å­—ä½“è®¾ç½®æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ ä¸­æ–‡å­—ä½“è®¾ç½®å¤±è´¥: {e}")
        print("å°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")

# åˆå§‹åŒ–æ—¶è®¾ç½®å­—ä½“
setup_chinese_font()

class ConsistencyDataAnalyzer:
    """ä½ç½®ä¸€è‡´æ€§æ•°æ®åˆ†æå™¨"""
    
    def __init__(self, data_file=None, data_dict=None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        å‚æ•°:
            data_file: JSONæ•°æ®æ–‡ä»¶è·¯å¾„
            data_dict: ç›´æ¥ä¼ å…¥çš„æ•°æ®å­—å…¸
        """
        self.data = None
        self.guide_positions = {}
        self.consistency_results = {}
        self.analysis_summary = {}
        
        if data_dict:
            self.load_data_from_dict(data_dict)
        elif data_file:
            self.load_data_from_file(data_file)
        else:
            raise ValueError("å¿…é¡»æä¾›data_fileæˆ–data_dictå‚æ•°")
    
    def load_data_from_file(self, file_path):
        """ä»æ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            self._extract_data()
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def load_data_from_dict(self, data_dict):
        """ä»å­—å…¸åŠ è½½æ•°æ®"""
        try:
            self.data = data_dict
            self._extract_data()
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®å­—å…¸")
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å­—å…¸å¤±è´¥: {e}")
            raise
    
    def _extract_data(self):
        """æå–æ•°æ®"""
        if not self.data:
            raise ValueError("æ²¡æœ‰æ•°æ®å¯æå–")
        
        self.guide_positions = self.data.get('guide_positions', {})
        self.consistency_results = self.data.get('consistency_results', {})
        self.analysis_summary = self.data.get('analysis_summary', {})
        
        print(f"ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        print(f"   - ä½ç½®æ•°é‡: {len(self.guide_positions)}")
        print(f"   - æœ‰æ•°æ®çš„ä½ç½®: {len(self.consistency_results)}")
        print(f"   - æ—¶é—´æˆ³: {self.data.get('timestamp', 'æœªçŸ¥')}")
    
    def validate_data(self):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        print("\nğŸ” æ•°æ®éªŒè¯:")
        
        issues = []
        
        # æ£€æŸ¥ä½ç½®æ•°æ®
        for pos_id, pos_info in self.guide_positions.items():
            if pos_id not in self.consistency_results:
                issues.append(f"ä½ç½® {pos_id} æ²¡æœ‰æµ‹é‡æ•°æ®")
        
        # æ£€æŸ¥æµ‹é‡æ•°æ®
        for pos_id, pos_results in self.consistency_results.items():
            if not pos_results:
                issues.append(f"ä½ç½® {pos_id} çš„æµ‹é‡ç»“æœä¸ºç©º")
                continue
            
            for weight_id, result in pos_results.items():
                if 'sensitivity_total' not in result:
                    issues.append(f"ä½ç½® {pos_id} ç ç  {weight_id} ç¼ºå°‘æ•æ„Ÿæ€§æ•°æ®")
                
                if 'cv' not in result:
                    issues.append(f"ä½ç½® {pos_id} ç ç  {weight_id} ç¼ºå°‘å˜å¼‚ç³»æ•°æ•°æ®")
        
        if issues:
            print("âš ï¸ å‘ç°ä»¥ä¸‹é—®é¢˜:")
            for issue in issues:
                print(f"   - {issue}")
        else:
            print("âœ… æ•°æ®éªŒè¯é€šè¿‡")
        
        return len(issues) == 0
    
    def get_data_summary(self):
        """è·å–æ•°æ®æ‘˜è¦"""
        print("\nğŸ“ˆ æ•°æ®æ‘˜è¦:")
        
        # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
        total_positions = len(self.guide_positions)
        measured_positions = len(self.consistency_results)
        total_measurements = 0
        valid_measurements = 0
        
        # æ”¶é›†æ‰€æœ‰æ•æ„Ÿæ€§æ•°æ®
        all_sensitivities = []
        all_cvs = []
        position_sensitivities = {}
        
        for pos_id, pos_results in self.consistency_results.items():
            position_sensitivities[pos_id] = []
            
            for weight_id, result in pos_results.items():
                total_measurements += 1
                
                sensitivity = result.get('sensitivity_total', 0)
                cv = result.get('cv', 0)
                
                all_sensitivities.append(sensitivity)
                all_cvs.append(cv)
                position_sensitivities[pos_id].append(sensitivity)
                
                if abs(sensitivity) > 1e-6:  # æœ‰æ•ˆæ•°æ®é˜ˆå€¼
                    valid_measurements += 1
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if all_sensitivities:
            avg_sensitivity = np.mean(all_sensitivities)
            std_sensitivity = np.std(all_sensitivities)
            min_sensitivity = np.min(all_sensitivities)
            max_sensitivity = np.max(all_sensitivities)
            
            avg_cv = np.mean(all_cvs)
            std_cv = np.std(all_cvs)
            
            print(f"   - æ€»æµ‹é‡ç‚¹: {total_measurements}")
            print(f"   - æœ‰æ•ˆæµ‹é‡ç‚¹: {valid_measurements} ({valid_measurements/total_measurements*100:.1f}%)")
            print(f"   - å¹³å‡æ•æ„Ÿæ€§: {avg_sensitivity:.6f} Â± {std_sensitivity:.6f}")
            print(f"   - æ•æ„Ÿæ€§èŒƒå›´: [{min_sensitivity:.6f}, {max_sensitivity:.6f}]")
            print(f"   - å¹³å‡å˜å¼‚ç³»æ•°: {avg_cv:.3f} Â± {std_cv:.3f}")
        
        return {
            'total_positions': total_positions,
            'measured_positions': measured_positions,
            'total_measurements': total_measurements,
            'valid_measurements': valid_measurements,
            'avg_sensitivity': avg_sensitivity if all_sensitivities else 0,
            'std_sensitivity': std_sensitivity if all_sensitivities else 0,
            'avg_cv': avg_cv if all_cvs else 0,
            'std_cv': std_cv if all_cvs else 0
        }
    
    def analyze_position_consistency(self):
        """åˆ†æä½ç½®ä¸€è‡´æ€§"""
        print("\nğŸ¯ ä½ç½®ä¸€è‡´æ€§åˆ†æ:")
        
        # è®¡ç®—æ¯ä¸ªä½ç½®çš„å¹³å‡æ•æ„Ÿæ€§
        position_avg_sensitivities = {}
        position_avg_cvs = {}
        
        for pos_id, pos_results in self.consistency_results.items():
            sensitivities = []
            cvs = []
            
            for result in pos_results.values():
                sensitivity = result.get('sensitivity_total', 0)
                cv = result.get('cv', 0)
                
                if abs(sensitivity) > 1e-6:  # åªè€ƒè™‘æœ‰æ•ˆæ•°æ®
                    sensitivities.append(sensitivity)
                    cvs.append(cv)
            
            if sensitivities:
                position_avg_sensitivities[pos_id] = np.mean(sensitivities)
                position_avg_cvs[pos_id] = np.mean(cvs)
                
                pos_name = self.guide_positions.get(pos_id, {}).get('name', pos_id)
                print(f"   - {pos_name}: æ•æ„Ÿæ€§={position_avg_sensitivities[pos_id]:.6f}, CV={position_avg_cvs[pos_id]:.3f}")
        
        # è®¡ç®—ä½ç½®é—´ä¸€è‡´æ€§
        if position_avg_sensitivities:
            sensitivity_values = list(position_avg_sensitivities.values())
            position_consistency_cv = np.std(sensitivity_values) / abs(np.mean(sensitivity_values)) if np.mean(sensitivity_values) != 0 else 0
            
            print(f"\n   ğŸ“Š ä½ç½®é—´ä¸€è‡´æ€§CV: {position_consistency_cv:.3f}")
            
            if position_consistency_cv < 0.05:
                print("   âœ… ä½ç½®ä¸€è‡´æ€§: ä¼˜ç§€ (<5%)")
            elif position_consistency_cv < 0.1:
                print("   âœ… ä½ç½®ä¸€è‡´æ€§: è‰¯å¥½ (5-10%)")
            elif position_consistency_cv < 0.2:
                print("   âš ï¸ ä½ç½®ä¸€è‡´æ€§: ä¸€èˆ¬ (10-20%)")
            else:
                print("   âŒ ä½ç½®ä¸€è‡´æ€§: è¾ƒå·® (>20%)")
        
        return {
            'position_avg_sensitivities': position_avg_sensitivities,
            'position_avg_cvs': position_avg_cvs,
            'position_consistency_cv': position_consistency_cv if position_avg_sensitivities else 0
        }
    
    def identify_problematic_positions(self):
        """è¯†åˆ«é—®é¢˜ä½ç½®"""
        print("\nğŸ” é—®é¢˜ä½ç½®è¯†åˆ«:")
        
        problematic_positions = []
        
        for pos_id, pos_results in self.consistency_results.items():
            pos_name = self.guide_positions.get(pos_id, {}).get('name', pos_id)
            issues = []
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è´Ÿæ•æ„Ÿæ€§
            negative_sensitivities = []
            for weight_id, result in pos_results.items():
                sensitivity = result.get('sensitivity_total', 0)
                if sensitivity < -1e-6:
                    negative_sensitivities.append((weight_id, sensitivity))
            
            if negative_sensitivities:
                issues.append(f"å­˜åœ¨è´Ÿæ•æ„Ÿæ€§: {negative_sensitivities}")
            
            # æ£€æŸ¥å˜å¼‚ç³»æ•°
            high_cv_count = 0
            for result in pos_results.values():
                cv = result.get('cv', 0)
                if cv > 0.1:  # CV > 10%
                    high_cv_count += 1
            
            if high_cv_count > 0:
                issues.append(f"é«˜å˜å¼‚ç³»æ•°æµ‹é‡ç‚¹: {high_cv_count}/{len(pos_results)}")
            
            # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
            sensitivities = [result.get('sensitivity_total', 0) for result in pos_results.values()]
            if len(sensitivities) > 1:
                sensitivity_cv = np.std(sensitivities) / abs(np.mean(sensitivities)) if np.mean(sensitivities) != 0 else 0
                if sensitivity_cv > 0.5:  # æ•æ„Ÿæ€§CV > 50%
                    issues.append(f"æ•æ„Ÿæ€§ä¸ä¸€è‡´: CV={sensitivity_cv:.3f}")
            
            if issues:
                problematic_positions.append((pos_id, pos_name, issues))
                print(f"   âš ï¸ {pos_name} ({pos_id}):")
                for issue in issues:
                    print(f"      - {issue}")
        
        if not problematic_positions:
            print("   âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜ä½ç½®")
        
        return problematic_positions
    
    def create_visualization(self, save_path=None):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“Š åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        
        # å‡†å¤‡æ•°æ®
        positions = []
        position_names = []
        avg_sensitivities = []
        avg_cvs = []
        
        for pos_id, pos_results in self.consistency_results.items():
            sensitivities = []
            cvs = []
            
            for result in pos_results.values():
                sensitivity = result.get('sensitivity_total', 0)
                cv = result.get('cv', 0)
                
                if abs(sensitivity) > 1e-6:
                    sensitivities.append(sensitivity)
                    cvs.append(cv)
            
            if sensitivities:
                positions.append(pos_id)
                position_names.append(self.guide_positions.get(pos_id, {}).get('name', pos_id))
                avg_sensitivities.append(np.mean(sensitivities))
                avg_cvs.append(np.mean(cvs))
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Position Consistency Analysis Results', fontsize=16, fontweight='bold')
        
        # 1. ä½ç½®æ•æ„Ÿæ€§å¯¹æ¯”
        ax1 = axes[0, 0]
        bars1 = ax1.bar(range(len(positions)), avg_sensitivities, color='skyblue', alpha=0.7)
        ax1.set_title('Average Sensitivity by Position')
        ax1.set_xlabel('Position')
        ax1.set_ylabel('Average Sensitivity')
        ax1.set_xticks(range(len(positions)))
        ax1.set_xticklabels(position_names, rotation=45, ha='right')
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, value) in enumerate(zip(bars1, avg_sensitivities)):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_sensitivities)*0.01,
                    f'{value:.4f}', ha='center', va='bottom', fontsize=9)
        
        # 2. ä½ç½®å˜å¼‚ç³»æ•°å¯¹æ¯”
        ax2 = axes[0, 1]
        bars2 = ax2.bar(range(len(positions)), avg_cvs, color='lightcoral', alpha=0.7)
        ax2.set_title('Average Coefficient of Variation by Position')
        ax2.set_xlabel('Position')
        ax2.set_ylabel('Average CV')
        ax2.set_xticks(range(len(positions)))
        ax2.set_xticklabels(position_names, rotation=45, ha='right')
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, value) in enumerate(zip(bars2, avg_cvs)):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_cvs)*0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 3. æ•æ„Ÿæ€§åˆ†å¸ƒç›´æ–¹å›¾
        ax3 = axes[1, 0]
        all_sensitivities = []
        for pos_results in self.consistency_results.values():
            for result in pos_results.values():
                sensitivity = result.get('sensitivity_total', 0)
                if abs(sensitivity) > 1e-6:
                    all_sensitivities.append(sensitivity)
        
        if all_sensitivities:
            ax3.hist(all_sensitivities, bins=20, color='lightgreen', alpha=0.7, edgecolor='black')
            ax3.set_title('Sensitivity Distribution (All Positions)')
            ax3.set_xlabel('Sensitivity')
            ax3.set_ylabel('Frequency')
            ax3.grid(True, alpha=0.3)
            
            # æ·»åŠ æ ‡å‡†å·®ä¿¡æ¯
            std_val = np.std(all_sensitivities)
            ax3.text(0.02, 0.98, f'Std: {std_val:.4f}', transform=ax3.transAxes, 
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        # 4. ä½ç½®-ç ç çƒ­åŠ›å›¾ - ä¿®å¤ç‰ˆæœ¬
        ax4 = axes[1, 1]
        
        # åˆ›å»ºä½ç½®-ç ç çŸ©é˜µ
        weight_ids = set()
        for pos_results in self.consistency_results.values():
            weight_ids.update(pos_results.keys())
        weight_ids = sorted(list(weight_ids))
        
        if positions and weight_ids:
            consistency_matrix = np.zeros((len(positions), len(weight_ids)))
            
            for i, pos_id in enumerate(positions):
                for j, weight_id in enumerate(weight_ids):
                    if (pos_id in self.consistency_results and 
                        weight_id in self.consistency_results[pos_id]):
                        consistency_matrix[i, j] = self.consistency_results[pos_id][weight_id].get('sensitivity_total', 0)
            
            # ä¿®å¤ï¼šè½¬ç½®çŸ©é˜µå¹¶åè½¬Yè½´ï¼Œä½¿ä½ç½®åœ¨Yè½´ä»ä¸‹åˆ°ä¸Šæ˜¾ç¤º
            consistency_matrix = consistency_matrix.T  # è½¬ç½®çŸ©é˜µ
            
            # åˆ›å»ºçƒ­åŠ›å›¾ï¼Œä½¿ç”¨origin='lower'ç¡®ä¿Yè½´ä»ä¸‹åˆ°ä¸Š
            im = ax4.imshow(consistency_matrix, cmap='plasma', aspect='auto', origin='lower')
            ax4.set_title('Position-Weight Sensitivity Heatmap')
            ax4.set_xlabel('Position')
            ax4.set_ylabel('Weight ID')
            ax4.set_xticks(range(len(positions)))
            ax4.set_xticklabels(position_names, rotation=45, ha='right')
            ax4.set_yticks(range(len(weight_ids)))
            ax4.set_yticklabels(weight_ids)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾ - æ³¨æ„åæ ‡å·²ç»è½¬ç½®
            for i in range(len(weight_ids)):
                for j in range(len(positions)):
                    value = consistency_matrix[i, j]
                    if abs(value) > 1e-6:
                        text_color = 'white' if value < np.mean(consistency_matrix) else 'black'
                        ax4.text(j, i, f'{value:.3f}', ha='center', va='center', 
                                color=text_color, fontsize=8, fontweight='bold')
            
            # æ·»åŠ é¢œè‰²æ¡
            cbar = plt.colorbar(im, ax=ax4)
            cbar.set_label('Sensitivity')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()
    
    def export_analysis_report(self, output_path):
        """å¯¼å‡ºåˆ†ææŠ¥å‘Š"""
        print(f"\nğŸ“„ å¯¼å‡ºåˆ†ææŠ¥å‘Šåˆ°: {output_path}")
        
        # è·å–åˆ†æç»“æœ
        summary = self.get_data_summary()
        consistency_analysis = self.analyze_position_consistency()
        problematic_positions = self.identify_problematic_positions()
        
        # åˆ›å»ºæŠ¥å‘Š
        report = []
        report.append("=" * 60)
        report.append("ä¼ æ„Ÿå™¨ä½ç½®ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æ•°æ®æ—¶é—´: {self.data.get('timestamp', 'æœªçŸ¥')}")
        report.append("")
        
        # æ•°æ®æ¦‚è§ˆ
        report.append("ğŸ“Š æ•°æ®æ¦‚è§ˆ")
        report.append("-" * 30)
        report.append(f"æ€»ä½ç½®æ•°: {summary['total_positions']}")
        report.append(f"æµ‹é‡ä½ç½®æ•°: {summary['measured_positions']}")
        report.append(f"æ€»æµ‹é‡ç‚¹: {summary['total_measurements']}")
        report.append(f"æœ‰æ•ˆæµ‹é‡ç‚¹: {summary['valid_measurements']} ({summary['valid_measurements']/summary['total_measurements']*100:.1f}%)")
        report.append("")
        
        # ç»Ÿè®¡ç»“æœ
        report.append("ğŸ“ˆ ç»Ÿè®¡ç»“æœ")
        report.append("-" * 30)
        report.append(f"å¹³å‡æ•æ„Ÿæ€§: {summary['avg_sensitivity']:.6f} Â± {summary['std_sensitivity']:.6f}")
        report.append(f"å¹³å‡å˜å¼‚ç³»æ•°: {summary['avg_cv']:.3f} Â± {summary['std_cv']:.3f}")
        report.append(f"ä½ç½®é—´ä¸€è‡´æ€§CV: {consistency_analysis['position_consistency_cv']:.3f}")
        report.append("")
        
        # ä½ç½®è¯¦ç»†åˆ†æ
        report.append("ğŸ¯ ä½ç½®è¯¦ç»†åˆ†æ")
        report.append("-" * 30)
        for pos_id, avg_sens in consistency_analysis['position_avg_sensitivities'].items():
            pos_name = self.guide_positions.get(pos_id, {}).get('name', pos_id)
            avg_cv = consistency_analysis['position_avg_cvs'].get(pos_id, 0)
            report.append(f"{pos_name}: æ•æ„Ÿæ€§={avg_sens:.6f}, CV={avg_cv:.3f}")
        report.append("")
        
        # é—®é¢˜ä½ç½®
        if problematic_positions:
            report.append("âš ï¸ é—®é¢˜ä½ç½®")
            report.append("-" * 30)
            for pos_id, pos_name, issues in problematic_positions:
                report.append(f"{pos_name} ({pos_id}):")
                for issue in issues:
                    report.append(f"  - {issue}")
            report.append("")
        
        # å»ºè®®
        report.append("ğŸ’¡ å»ºè®®")
        report.append("-" * 30)
        if consistency_analysis['position_consistency_cv'] > 0.2:
            report.append("- ä½ç½®ä¸€è‡´æ€§è¾ƒå·®ï¼Œå»ºè®®æ£€æŸ¥ä¼ æ„Ÿå™¨æ ¡å‡†")
        if summary['valid_measurements'] / summary['total_measurements'] < 0.8:
            report.append("- æœ‰æ•ˆæ•°æ®æ¯”ä¾‹è¾ƒä½ï¼Œå»ºè®®é‡æ–°æµ‹é‡")
        if problematic_positions:
            report.append("- å­˜åœ¨è´Ÿæ•æ„Ÿæ€§æ•°æ®ï¼Œå»ºè®®æ£€æŸ¥åŸºçº¿æ ¡æ­£")
        if not problematic_positions and consistency_analysis['position_consistency_cv'] < 0.1:
            report.append("- ä½ç½®ä¸€è‡´æ€§è‰¯å¥½ï¼Œä¼ æ„Ÿå™¨æ€§èƒ½ç¨³å®š")
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
    
    def run_full_analysis(self, output_dir="analysis_results"):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹å®Œæ•´åˆ†æ...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # æ•°æ®éªŒè¯
        is_valid = self.validate_data()
        
        # è·å–åˆ†æç»“æœ
        summary = self.get_data_summary()
        consistency_analysis = self.analyze_position_consistency()
        problematic_positions = self.identify_problematic_positions()
        
        # åˆ›å»ºå¯è§†åŒ–
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plot_path = os.path.join(output_dir, f"consistency_analysis_{timestamp}.png")
        self.create_visualization(plot_path)
        
        # å¯¼å‡ºæŠ¥å‘Š
        report_path = os.path.join(output_dir, f"analysis_report_{timestamp}.txt")
        self.export_analysis_report(report_path)
        
        print(f"\nâœ… å®Œæ•´åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š å›¾è¡¨: {plot_path}")
        print(f"ğŸ“„ æŠ¥å‘Š: {report_path}")
        
        return {
            'is_valid': is_valid,
            'summary': summary,
            'consistency_analysis': consistency_analysis,
            'problematic_positions': problematic_positions,
            'plot_path': plot_path,
            'report_path': report_path
        }


def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    # ç¤ºä¾‹æ•°æ®ï¼ˆæ‚¨å¯ä»¥å°†æ‚¨çš„æ•°æ®ç²˜è´´åœ¨è¿™é‡Œï¼‰
    sample_data = {
        "timestamp": "2025-08-01T09:34:55.048622",
        "guide_positions": {
            "center": {"name": "ä¸­å¿ƒä½ç½®", "x": 32, "y": 32, "description": "ä¼ æ„Ÿå™¨ä¸­å¿ƒä½ç½®"}
        },
        "consistency_results": {
            "center": {
                "1": {
                    "weight_info": {"mass": 50.0, "unit": "g", "force": 0.49},
                    "measurement_count": 10,
                    "avg_total_pressure": 0.005066163004057334,
                    "std_total_pressure": 0.00019036257213867988,
                    "cv": 0.03757529554146291,
                    "sensitivity_total": 0.01033910817154558
                }
            }
        },
        "analysis_summary": {
            "avg_cv": 0.029305598248228797,
            "std_cv": 0.07440675621285253,
            "avg_sensitivity": -0.02002797769033922,
            "std_sensitivity": 0.04060561137120782,
            "position_consistency_cv": 0
        }
    }
    
    print("ğŸ”§ ä½ç½®ä¸€è‡´æ€§æ•°æ®åˆ†æå·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ConsistencyDataAnalyzer(data_dict=sample_data)
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    results = analyzer.run_full_analysis()
    
    print("\nğŸ“‹ åˆ†æç»“æœæ‘˜è¦:")
    print(f"æ•°æ®æœ‰æ•ˆæ€§: {'âœ… é€šè¿‡' if results['is_valid'] else 'âŒ å¤±è´¥'}")
    print(f"æœ‰æ•ˆæµ‹é‡ç‚¹: {results['summary']['valid_measurements']}/{results['summary']['total_measurements']}")
    print(f"ä½ç½®ä¸€è‡´æ€§CV: {results['consistency_analysis']['position_consistency_cv']:.3f}")
    print(f"é—®é¢˜ä½ç½®æ•°: {len(results['problematic_positions'])}")


if __name__ == "__main__":
    main() 